## 1. 树模型
>https://blog.csdn.net/u012328159/article/details/70184415
- 基础决策树模型：ID3，C4.5，CART
- 分裂节点的选择 - 信息增益 - 信息增益比 - 基尼指数
### （1）不同节点分裂选择的比较
- 信息增益：即信息增益对可取值数目多的特征有偏好（即该属性能取得值越多，信息增益，越偏向这个），因为特征可取的值越多，会导致“纯度”越大，即ent（D）会很小，如果一个特征的离散个数与样本数相等，那么Ent（D）值会为0
- 信息增益率：对可取数值数目较少的属性有所偏好。
### （2）剪枝
- 预剪枝（pre-pruning）：预剪枝就是在构造决策树的过程中，先对每个结点在划分前进行估计，若果当前结点的划分不能带来决策树模型泛华性能的提升，则不对当前结点进行划分并且将当前结点标记为叶结点。
>预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。但是，另一方面，因为预剪枝是基于“贪心”的，所以，虽然当前划分不能提升泛华性能，但是基于该划分的后续划分却有可能导致性能提升，因此预剪枝决策树有可能带来欠拟合的风险。
- 后剪枝（post-pruning）：后剪枝就是先把整颗决策树构造完毕，然后自底向上的对非叶结点进行考察，若将该结点对应的子树换为叶结点能够带来泛华性能的提升，则把该子树替换为叶结点。
>后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛华性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。
### （3）连续值处理
常用的离散化策略是二分法，这个技术也是C4.5中采用的策略。
（取数据中位点）
### （4）缺失值处理
C4.5中采用的方法是：测试样本在该属性值上有缺失值，那么就同时探查（计算）所有分支，然后算每个类别的概率，取概率最大的类别赋值给该样本

## 2. 集成学习 - bagging：随机森林 - 随机森林怎么防止过拟合
### 1.随机森林
- **特点**
在当前所有算法中，具有极好的准确率
能够有效地运行在大数据集上
能够处理具有高维特征的输入样本，而且不需要降维；
能够评估各个特征在分类问题上的重要性；
在生成过程中，能够获取到内部生成误差的一种无偏估计；
对于缺省值问题也能够获得很好得结果
- **要点**
（1）如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法）；
（2）如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的
**两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）.**
（3）每棵树都尽最大程度的生长，并且没有剪枝过程。
（4）袋外错误率
>在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。
袋外错误率估计，它的计算方式如下：
(1)对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；
(2)然后以简单多数投票作为该样本的分类结果；
(3)最后用误分个数占样本总数的比率作为随机森林的oob误分率。

### 2.随机森林如何处理缺失值
- 首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值
- 然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径.
- 判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N
- 如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。

### 3.优缺点
随机森林的优点：
- 随机森林仅需要指定两个参数，学习和使用更简单，预测精度高、训练速度快；
- 随机森林引入两个随机性，使得随机森林具有良好的抗噪声能力，不容易出现过拟合；
- 随机森林可以在分类中对各个变量的重要性进行估计，可以在创建过程中给出对泛化误差的无偏估计；
- 随机森林可以处理很高维的数据，对数据集的适应能力强；
- 随机森林的预测效果和性能稳定性要优于许多单预测器和集成预测方法，分类精度可与Boosting方法（如AdaBoost）相媲美，并且运行速度更快。
2、随机森林的缺点： 
- 随机森林在训练数据较少时会可能导致过拟合

## 3.Boosting提升算法
Boosting算法是将“弱学习算法“提升为“强学习算法”的过程，主要思想是“三个臭皮匠顶个诸葛亮”。
>https://www.cnblogs.com/ScorpioLu/p/8295990.html
### （1）AdaBoost
AdaBoost就是损失函数为指数损失的Boosting算法
- **原理**
**一次迭代的弱学习h(x;am)有何不一样，如何学习？**
AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。然后，再根据所采用的一些基本机器学习算法进行学习，比如逻辑回归。
**弱分类器权值βm如何确定**
AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。
- **Adaboost和GBDT的异同点**
1)关注点：分错权值，残差
Adaboost每轮学习的一个基本学习器是通过改变样本的权值，关注上轮分类错误的样本的权值，以逐步减少在训练集上的分类误差率。而GBDT每轮学习一个基本学习器是通过改变输出值，每轮拟合的值为真实值与已有的加法模型的差值（即残差）。
2)异常点:adaboost存在异常点敏感的问题,gbdt一定程度上优化了adaboost异常点敏感的问题，但是存在难以并行的缺点
3）树:GBDT无论是进行分类还是回归问题，都用的CART树，对分类问题用二叉分类树，回归问题用二叉回归树。
4）方差偏差:两者的目标都是优化bias，必然导致训练出来的数据var的不稳定

## 4. 梯度提升树（GDBT）
>https://www.cnblogs.com/ModifyRong/p/7744987.html
### (1) 训练过程
- gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法
- 弱分类器一般会选择为CART TREE（也就是分类回归树）
- 让损失函数沿着梯度方向的下降。这个就是gbdt 的 gb的核心了。 利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值去拟合一个回归树。gbdt 每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度。
- 这样每轮训练的时候都能够让损失函数尽可能快的减小，尽快的收敛达到局部最优解或者全局最优解。
### (2) 特征选择
gbdt选择特征的细节其实是想问你CART Tree生成的过程。这里有一个前提，gbdt的弱分类器默认选择的是CART TREE
### (3) 构建特征
- gbdt 本身是不能产生特征的，但是我们可以利用gbdt去产生特征的组合
- 工业界一般会采用逻辑回归去进行处理:
逻辑回归本身是适合处理线性可分的数据，如果我们想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力
### (4) 分类
- gbdt 无论用于分类还是回归一直都是使用的CART 回归树。不会因为我们所选择的任务是分类任务就选用分类树，这里面的核心是因为gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的
- 我们在训练的时候，是针对样本 X 每个可能的类都训练一个分类回归树
### (5) 并行计算
GBDT部分可并行计算
- 计算(更新)每个样本的负梯度。
- 在拟合残差树时某个节点分类时，求各种可能的分类情况时的增益(基尼系数)是可以并行计算的。(GBDT没有做，xgboost做到了，用一个block结构存储这些特征增益)。但对于拟合一整棵残差树，增益是无法并行计算的
- 最后预测过程中，每个样本将之前的所有树的结果累加的时候。
### （6）GBDT 为什么用梯度拟合残差
- 负梯度的方向可证，模型优化下去一定会收敛
- 对于一些损失函数来说最大的残差方向，并不是梯度下降最好的方向，倒是损失函数最小与残差最小两者目标不统一
### 优缺点
优点
- 可以灵活处理各种类型的数据，包括连续值和离散值。
- 使用一些比较strong的损失函数，比如Huber损失函数和Quantile损失函数，对异常值的鲁棒性非常强。
- 相对于SVM来说，在相对较少的调参时间下，预测的准确度较高。
缺点：
- 由于弱学习器之间存在依赖关系，难以并行训练数据。
##几个问题
1）为什么建树采用ensemble决策树
一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。
2）为什么建树采用GBDT而非RF？
RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征
> 原文链接：https://blog.csdn.net/lilyth_lilyth/article/details/48032119


## 5.XGBoost
### (1)XGBoost 目标函数？
右边第一部分是模型的训练误差，第二部分是正则化项，这里的正则化项是K棵树的正则化项相加而来的
### (2)XGBoost 怎么做并行？
>https://blog.csdn.net/anshuai_aw1/article/details/85093106
##### 分块并行
XGBoost在训练之前，预先对数据进行了排序，然后保存为block(块)结构，后⾯面的迭代中重复地使⽤用这个结构，⼤大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
- Block中的数据以稀疏格式CSC进行存储。CSC格式请参考该文章。
>https://www.cnblogs.com/rollenholt/p/5960523.html
- Block中的特征进行排序（不对缺失值排序），且只需要排序一次，以后分裂树的过程可以复用
##### 缓存优化
- 使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的。这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率
- 对每个线程分配一个连续的buffer，读取梯度信息并存入Buffer中（这样就实现了非连续到连续的转化），然后再统计梯度信息。
#####out-of-core
当数据量太大不能全部放入主内存的时候，为了使得out-of-core计算称为可能，将数据划分为多个Block并存放在磁盘上。
- Block压缩（Block Compression）：将Block按列压缩（LZ4压缩算法？），读取的时候用另外的线程解压。
- Block拆分（Block Sharding）：将数据划分到不同磁盘上，为每个磁盘分配一个预取（pre-fetcher）线程，并将数据提取到内存缓冲区中。然后，训练线程交替地从每个缓冲区读取数据。这有助于在多个磁盘可用时增加磁盘读取的吞吐量。
### (3)XGBoost 与 传统 GBDT 区别？
##### 算法层面
- 1.XGB加了正则项，普通GBDT没有。为了防止过拟合(叶子节点数及权重)
- 2.xgboost损失函数是误差部分是二阶泰勒展开，GBDT 是一阶泰勒展开。因此损失函数近似的更精准。
- 3.对每颗子树增加一个参数，使得每颗子树的权重降低，防止过拟合，这个参数叫shrinkage
对特征进行降采样，灵感来源于随机森林，除了能降低计算量外，还能防止过拟合。
- 4.实现了利用分捅/分位数方法，实现了全局和局部的近似分裂点算法，降低了计算量，并且在eps参数设置合理的情况下，能达到穷举法几乎一样的性能
- 5.增加处理缺失值的方案（通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向）。
##### 系统层面
- 对每个特征进行分块（block）并排序，使得在寻找最佳分裂点的时候能够并行化计算。这是xgboost比一般GBDT更快的一个重要原因
- 通过设置合理的block的大小，充分利用了CPU缓存进行读取加速（cache-aware access）。使得数据读取的速度更快。因为太小的block的尺寸使得多线程中每个线程负载太小降低了并行效率。太大的block尺寸会导致CPU的缓存获取miss掉。
- out-of-core 通过将block压缩（block compressoin）并存储到硬盘上，并且通过将block分区到多个硬盘上（block Sharding）实现了更大的IO 读写速度，
### (4)XGBoost 对特征缺失敏感么？对缺失值做了什么操作？哪些模型对缺失值敏感？哪些不敏感？
通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向

## 6.LightGBM
- histogram算法替换了传统的Pre-Sorted，某种意义上是牺牲了精度（但是作者声明实验发现精度影响不大）换取速度，直方图作差构建叶子直方图挺有创造力的。（xgboost的分布式实现也是基于直方图的，利于并行）
- 带有深度限制的按叶子生长 (leaf-wise) 算法代替了传统的(level-wise) 决策树生长策略，提升精度，同时避免过拟合危险。