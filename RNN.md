# 1.RNN 的基本结构
循环神经网络，Recurrent Neural Network

# 2.相关问题
### （1）RNN 相比前馈网络/CNN 有什么特点
##### 前馈网络/CNN 处理序列数据时存在的问题
- 一般的前馈网络，通常接受一个定长向量作为输入，然后输出一个定长的表示；它需要一次性接收所有的输入，因而忽略了序列中的顺序信息；
- CNN 在处理变长序列时，通过滑动窗口+池化的方式将输入转化为一个定长的向量表示，这样做可以捕捉到序列中的一些局部特征，但是很难学习到序列间的长距离依赖。
##### RNN 处理时序数据时的优势
- RNN 很适合处理序列数据，特别是带有时序关系的序列，比如文本数据；
- RNN 把每一个时间步中的信息编码到状态变量中，使网络具有一定的记忆能力，从而更好的理解序列信息。
- 由于 RNN 具有对序列中时序信息的刻画能力，因此在处理序列数据时往往能得到更准确的结果。
### （2）RNN 为什么会出现梯度消失/梯度爆炸
最大步长为 T 的 RNN 展开后相当于一个共享参数的 T 层前馈网络
##### RNN 中能否使用 ReLU 作为激活函数
- 答案是肯定的。但是会存在一些问题
- 假设使用 ReLU 并始终处于激活状态（a_{t-1} > 0），则 f(x) = x，按照以上的步骤继续展开，最终结果中将包含 t 个 W 连乘，如果 W 不是单位矩阵，最终结果将趋于 0 或无穷。
- 使用 ReLU 也不能完全避免 RNN 中的梯度消失/爆炸问题，问题依然在于存在 t 个 W 的连乘项

##### 梯度爆炸的解决方法
梯度截断
##### 梯度消失的解决方法（针对 RNN）
残差结构
门控机制（LSTM、GRU

# 3. LSTM 相关问题
### （1）LSTM的内部结构
- LSTM 在传统 RNN 的基础上加入了门控机制来限制信息的流动。
- 具体来说，LSTM 中加入了三个“门”：遗忘门 f、输入门 i、输出门 o，以及一个内部记忆状态 C
### （2）LSTM 是如何实现长短期记忆的？（遗忘门和输入门的作用）
LSTM 主要通过遗忘门和输入门来实现长短期记忆。
### （3）LSTM 里各部分使用了不同的激活函数，为什么？可以使用其他激活函数吗？
- 在 LSTM 中，所有控制门都使用 sigmoid 作为激活函数（遗忘门、输入门、输出门）；
- 在计算候选记忆或隐藏状态时，使用双曲正切函数 tanh 作为激活函数

**sigmoid 的“饱和”性**

所谓饱和性，即输入超过一定范围后，输出几乎不再发生明显变化了

**为什么使用 tanh？**
使用 tanh 作为计算状态时的激活函数，主要是因为其值域为 (-1, 1)：
- 一方面，这与多数场景下特征分布以 0 为中心相吻合；
- 另一方面，可以避免在前向传播的时候发生数值问题（主要是上溢）

此外，tanh 比 sigmoid 在 0 附近有更大的梯度，通常会使模型收敛更快。
### (4) GRU 与 LSTM 的关系
- GRU 认为 LSTM 中的遗忘门和输入门的功能有一定的重合，于是将其合并为一个更新门。
- 并使用重置门（reset） r 代替输出门；