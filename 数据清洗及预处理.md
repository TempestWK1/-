## 数据缺失值处理
##### 删除元组
- 在对象有多个属性缺失值、被删除的含缺失值的对象与信息表中的数据量相比非常小的情况下是非常有效的，类标号（假设是分类任务）缺少时通常使用。
- 当遗漏数据所占比例较大，特别当遗漏数据非随机分布时，这种方法可能导致数据发生偏离，从而引出错误的结论。
##### 数据补齐
- 平均值填充
将信息表中的属性分为**数值属性**和**非数值属性**来分别进行处理。如果空值是**数值型**的，就根据该属性在其他所有对象的取值的**平均值**来填充该缺失的属性值；如果空值是**非数值型**的，就根据统计学中的**众数原理**，用该属性在其他所有对象的取值次数最多的值(即出现频率最高的值)来补齐该缺失的属性值。
- K最近距离邻法
先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本，将这K个值加权平均来估计该样本的缺失数据。
- 回归
- 就近补齐
- 期望值最大化方法
在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计。
适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂。

## 归一化
数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。
##### 好处
- 提升模型的收敛速度
- 提升模型的精度
归一化的另一好处是提高精度，这在涉及到一些距离计算的算法时效果显著，比如算法要计算欧氏距离，上图中x2的取值范围比较小，涉及到距离计算时其对结果的影响远比x1带来的小，所以这就会造成精度的损失。所以归一化很有必要，他可以让各个特征对结果做出的贡献相同
##### 需要归一化的模型：
Adaboost、SVM、LR、Knn、KMeans之类的最优化问题就需要归一化
- 用了梯度下降:迭代算法可能收敛得很慢甚至不收敛
- 和距离有关的：有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价
##### 不需要归一化的模型
概率模型（树形模型）不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF

## 如何解决机器学习中数据不平衡问题
##### 采样
采样分为**上采样**（Oversampling）和**下采样**（Undersampling），上采样是把小种类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本
- 上采样会把小众样本复制多份，一个点会在高维空间中反复出现，这会导致一个问题，那就是运气好就能分对很多点，否则分错很多点。为了解决这一问题，可以在每次生成新数据点时加入轻微的随机扰动，经验表明这种做法非常有效。
- 下采样会丢失信息，如何减少信息的损失呢？第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果
##### 数据合成
数据合成方法是利用已有样本生成更多样本，这类方法在小数据场景下有很多成功案例
##### 加权
除了采样和生成新数据等方法，我们还可以通过加权的方式来解决数据不平衡问题，即对不同类别分错的代价不同
##### 一分类
对于正负样本极不平衡的场景，我们可以换一个完全不同的角度来看待问题：把它看做一分类（One Class Learning）或异常检测（Novelty Detection）问题。这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模，经典的工作包括One-class SVM等。
### 如何选择
- 在正负样本都非常之少的情况下，应该采用数据合成的方式；
- 在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法；
- 在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。