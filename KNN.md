#KNN
k近邻法（k-nearest neighbor, kNN）是一种基本**分类与回归方法**，其基本做法是：给定测试实例，基于某种距离度量找出训练集中与其最靠近的k个实例点，然后基于这k个最近邻的信息来进行预测。
##### 三要素：距离度量、k值的选择、分类决策规则。
根据选择的距离度量（如曼哈顿距离或欧氏距离），可计算测试实例与训练集中的每个实例点的距离，根据k值选择k个最近邻点，最后根据分类决策规则将测试实例分类。
- 距离度量
曼哈顿距离、欧氏距离
- k值的选择
k值的选择会对k近邻法的结果产生重大影响。在应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的k值
- 分类决策规则
k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类，决定输入实例的类。

## 1.数据归一化
因为不同维度的物理量之间往往具有不同的量纲和单位，比如时间，距离等，这样会造成维度之间可比性较差，为了消除物理量之间绝对值相差太大，需要对样本数据集进行标准化处理，保证各个物理量之间处于同一个数量级之下，消除不同量纲之间的差异，一般有如下两个常用的归一化方法：

## 2.KNN算法的优缺点
- **优点**
非常简单的分类算法没有之一，人性化，易于理解，易于实现
适合处理多分类问题，比如推荐用户
- **缺点**
1.属于懒惰算法，时间复杂度较高，因为需要计算未知样本到所有已知样本的距离
2.样本平衡度依赖高，当出现极端情况样本不平衡时，分类绝对会出现偏差
3.可解释性差，无法给出类似决策树那样的规则
4.向量的维度越高，欧式距离的区分能力就越弱

## 3.与EM算法的关系
K均值算法和EM算法的相似之处在于，K均值先固定中心，然后寻找最近邻的K个点，在根据K个点反过来计算最优中心

## 回归
- 计算待测点到已知点的距离
- 选择距离待测点最近的K个点，k值为人工设置的，至于k值如何设置合适在后边讨论。
- 将k个点的值取平均值作为最终的预测结果