## 梯度下降法
批量梯度下降法BGD，随机梯度下降法SGD，小批量梯度下降法MBGD
#### 1.批量梯度下降法BGD
批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新
- 优点：全局最优解；易于并行实现；
- 缺点：当样本数目很多时，训练过程会很慢。

从迭代的次数上来看，BGD迭代的次数相对较少。
#### 2.随机梯度下降法SGD
由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。
SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。
- 优点：训练速度快；
- 缺点：准确度下降，并不是全局最优；不易于并行实现。
从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。
#### 3.小批量梯度下降法MBGD
有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。
#### 4.Momentum梯度下降法
SGD、BSGD两种改进方法都存在不同程度的震荡，如何避免震荡？
- 当本次梯度下降方向与上次更新量的方向相同时，上次的更新量能够对本次的搜索起到一个正向加速的作用。
- 当本次梯度下降方向与上次更新量的方向相反时，上次的更新量能够对本次的搜索起到一个减速的作用。

### 为什么负梯度方向是目标函数下降最快的方向
>https://blog.csdn.net/u013166817/article/details/85131588

最小化损失函数: J(θ)。为了快速得到最佳的参数θ,我们需要找到损失函数下降最快的方向，即找到一个θ移动的方向v, 使得J(θ)−J(θ+v)最大。
对J(θ+v)进行一阶泰勒展开：
J(θ+v)≈J(θ)+v▽θJ(θ)
J(θ)−J(θ+v)≈-v▽θJ(θ)
则最大化 J(θ)−J(θ+v) 变为最大化 -v▽θJ(θ)
因为 -v▽θJ(θ)可以理解为两个向量的点积，为了使上述“负点积”最大，则两个向量应该方向相反（180度），即v和▽θJ(θ)方向相反

## 牛顿法
机器学习算法中经常碰到非线性优化问题，如 Sparse Filtering 算法，其主要工作在于求解一个非线性极小化问题。在具体实现中，大多调用的是成熟的软件包做支撑，其中最常用的一个算法是 L-BFGS。
>https://blog.csdn.net/qq_28743951/article/details/82221515
- **优点**
它比传统的梯度下降算法收敛速度明显要快，另外，当f(x)是二次函数时，仅需一次迭代就能直接收敛到最小值。因为f(x)为二次函数时，在进行泰勒展开式，并没有高阶导数的损失，而在后面的每次计算都是等号运算，因而最后得到结果就是函数f(x)最小值对应的x。而对于非二次函数，若函数的二次形态较强，或迭代点已进入极小点的领域内，则其收敛速度也会很快。
- **缺点**
**计算复杂度问题**：海塞矩阵的计算问题
（针对计算复杂度问题，于是有了拟牛顿法。）
**收敛性问题**：对于非二次函数，牛顿法可能不能收敛，从而导致计算失败
（对于这种情况，可以使用阻尼牛顿法来解决。）