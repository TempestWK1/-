# 1.自编码器
>https://www.cnblogs.com/royhoo/p/Autoencoders.html

- 自编码器是一种能够通过无监督学习，学到输入数据高效表示的人工神经网络。输入数据的这一高效表示称为编码（codings），其维度一般远小于输入数据，使得自编码器可用于降维。更重要的是，自编码器可作为强大的特征检测器（feature detectors），应用于深度神经网络的预训练.
- encoder（也称为识别网络）将输入转换成内部表示，decoder（也称为生成网络）将内部表示转换成输出。

# 2.因子分解机 Factorization Machine
》https://blog.csdn.net/ddydavie/article/details/82667890
- 因子分解机主要是考虑了特征之间的关联。

- FM主要是为了解决数据稀疏的情况下，（而SVM无法解决稀疏问题），特征怎样组合的问题。

- 数据稀疏是指数据的维度很大，但是其中为0的维度很多。推荐系统是常见应用场景，原因是推荐系统中类别属性（如商品id）比较多，每一种类别属性经过onehot处理后会产生大量值为0的特征，导致样本变得稀疏，而FM就可以解决这种样本稀疏的问题。
### FM模型
为了表述特征间的相关性，我们采用多项式模型，将特征xi和xj的组合用xixj表示，只讨论二阶多项式模型

# 3.反向传播算法
反向传播的 4 个基本公式

# 4.激活函数
##### 激活函数的作用——为什么要使用非线性激活函数
使用激活函数的目的是为了向网络中加入非线性因素；加强网络的表示能力，解决线性模型无法解决的问题

##### 为什么加入非线性因素能够加强网络的表示能力？——神经网络的万能近似定理
- 神经网络的万能近似定理认为主要神经网络具有至少一个非线性隐藏层，那么只要给予网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的函数。

- 如果不使用非线性激活函数，那么每一层输出都是上层输入的线性组合；

- 此时无论网络有多少层，其整体也将是线性的，这会导致失去万能近似的性质

- 但仅部分层是纯线性是可以接受的，这有助于减少网络中的参数。

### 常见的激活函数
- 整流线性单元 ReLU
- sigmoid 与 tanh
- 其他激活函数：线性激活函数，径向基函数，softplus

**ReLU 相比 sigmoid 的优势**
- 1.**避免梯度消失**：
sigmoid函数在输入取绝对值非常大的正值或负值时会出现饱和现象
ReLU 的导数始终是一个常数
- 2.**减缓过拟合**：ReLU 在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性
- 3.**加速计算**：
ReLU 的求导不涉及浮点运算，所以速度更快

# 5.正则化
### Batch Normalization（批标准化）
BN 是一种正则化方法（减少泛化误差），主要作用有：
- 加速网络的训练（缓解梯度消失，支持更大的学习率）
- 防止过拟合
- 降低了参数初始化的要求。
### 基本原理
- BN 方法会针对每一批数据，在网络的每一层输入之前增加归一化处理，目的是将数据限制在统一的分布下。
- 针对每层的第 k 个神经元，计算这一批数据在第 k 个神经元的均值与标准差，然后将归一化后的值作为该神经元的激活值。
- BN 可以看作在各层之间加入了一个新的计算层，对数据分布进行额外的约束，从而增强模型的泛化能力
- 但同时 BN 也降低了模型的拟合能力，破坏了之前学到的特征分布
- 为了恢复数据的原始分布，BN 引入了一个重构变换来还原最优的输入数据分布（其中 γ 和 β 为可训练参数。）

# 6.L1/L2 范数正则化
### 相同点
限制模型的学习能力——通过限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。
### 不同点
- L1 正则化可以产生更稀疏的权值矩阵，可以用于特征选择，同时一定程度上防止过拟合；L2 正则化主要用于防止模型过拟合
- L1 正则化适用于特征之间有关联的情况；L2 正则化适用于特征之间没有关联的情况。

# 7.Dropout
Dropout 通过参数共享提供了一种廉价的 Bagging 集成近似—— Dropout 策略相当于集成了包括所有从基础网络除去部分单元后形成的子网络。

### Dropout 与 Bagging 的不同
- 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。
- 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。

# 8.参数初始化
- 一般使用服从的高斯分布（mean=0, stddev=1）或均匀分布的随机值作为权重的初始化参数；使用 0 作为偏置的初始化参数
- 一些启发式方法会根据输入与输出的单元数来决定初始值的范围（比如glorot_uniform）